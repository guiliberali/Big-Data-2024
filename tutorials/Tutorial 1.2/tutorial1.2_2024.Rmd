---
title: 'Learning from Big Data: Tutorial 1.2'
author: "Tayyip Altan"
date: "September 2024"
output:
  pdf_document:
    fig_caption: yes
  html_document:
    df_print: paged
header-includes: "\\usepackage{float} \\usepackage{booktabs} % To thicken table lines
  \\usepackage{unicode-math}"
urlcolor: blue
---

# Introduction
This file illustrates LDA and word2vec using the review data.

# 1.  Loading libraries

Before starting the tutorial, make sure you have all the required libraries properly installed. 
Simply run this chunk of code below.

```{r, echo = T, results = 'hide', message=FALSE, warning = FALSE}

# Required packages. P_load ensures these will be installed and loaded.
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tm, openNLP, nnet, dplyr, tidyr, ggplot2, reshape2,
               latex2exp, topicmodels,word2vec,tokenizers)

```

# 2. Load the reviews
```{r, load data}
# Load the review data. Note that we are now using the fileEncoding parameter when
#      calling read.csv() - this helps reading the review text correctly for further 
#      processing (by correctly interpreting the non-ASCII symbols)
reviews_raw <- read.csv('Reviews_tiny.csv', fileEncoding="ISO-8859-1")

# Selecting only the relevant columns from the entire dataset
reviews_raw <- reviews_raw %>% 
               select(movie_name,review_code,	reviewer,	review_date, num_eval,
                      prob_sentiment,words_in_lexicon_sentiment_and_review, ratio_helpful,	
                      raters,   
                      prob_storyline,	prob_acting,	prob_sound_visual,
                      full_text,	processed_text,
                      release_date,	first_week_box_office,MPAA,	studio,	num_theaters )

# Determining the total number of reviews in our dataset
total_reviews <- nrow(reviews_raw) 


# Loading fake likelihoods data
likelihoods <- read.csv("example_100_fake_likelihood_topic.csv")

```

Inspect list of words to be passed to LDA: 
```{r, lexicon content, results='hide'}

# set out lexicon equal to the first column of the likelihoods data and inspecting its structure
lexicon_content  <- as.character(likelihoods[ ,1] )
str(lexicon_content)

```
 

# 3. Unsupervised Learning:  Latent Dirichlet Allocation (LDA)  

```{r, Preparing data for LDA}

 # Put processed reviews in corpus format
corpus_review <- VCorpus(VectorSource(reviews_raw$processed_text)) 

# Creates the document term matrix that will be passed to the LDA function
dtm <- DocumentTermMatrix(corpus_review, 
                       control = list(stemming = FALSE,  # Which is also the default
                                      language = "english",
                                      removePunctuation = TRUE,
                                      removeNumbers = TRUE,
                                      dictionary = as.character(lexicon_content)) )

# Inpsecting the structure of the DocumentTermMatrix
str(dtm)
```
```{r, results='hide'}
# Useful info on the DocumentTermMatrix and its parameters
?DocumentTermMatrix()
?termFreq()   # more info on the control settings used within the dtm
```

Next, we will set the LDA parameters. k is the number of topics we ask LDA to estimate. In supervised learning, we set that to 3. In this example, we arbitrarily set k at 10. Seed is for replicability (i.e., obtain the same random number every time the code is run). Burn-in and number of iterations are for convergence of the Markov chains in the Gibbs sampler (MCMC-based inference is outside of the scope of this course and not required for the assignment - just use the default values below.) In the unlikely case you have warnings re: not convergence, you can increase ITER to 2k or 4k.


```{r, LDA parameters}
# LDA parameters
seed <- 20080809 
burnin <-  1000 
iter <- 1000
k <- 10
```
Tip: choosing which k to use in LDA is a model selection problem. Typically, the best approach is to compute a model for each level of k, save the model log-likelihood (produced by applying the function logLik() to the model, stored in model_lda) and choosing the k that produced the  highest log-likelihood.

Next, we will run the LDA and save the model. The model produced by LDA() is an object of class LDA (page 11 of https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf ). This class includes the topics, the log-likelihood, and a lot more. To extract this information, it is needed to use functions listed in page 2 of the said pdf, as shown in the code below.

```{r, Computing LDA}
#Create an LDA model using GIBBS sampling
model_lda <- LDA(dtm, k, method = "Gibbs", control = list(seed = seed, burnin = burnin, iter = iter) , mc.cores = 4)
save(model_lda , file = paste("LDA_model_" ,k,".RData" ,sep="")) 

```

Inspect posteriors.
```{r, posterior}
#posterior probabilities per document by topic
posteriors_lda <- posterior(model_lda)$topics
str(posteriors_lda)
posteriors_lda[999,]
```

# 4. Unsupervised Learning:  word embeddings  

Our word embedding example has three steps. First, run word2vec to train a model using the training data split. Second, it uses the trained model to analyze the prediction data split. Third, it uses the constructed variables to forecast box office.

Step 1 - Training Step

```{r, Run word2vec - training}
# Obtain the column with the reviews and convert it to lower case
x <-  reviews_raw$full_text
x <- tolower(x)

# TODO: use a split of the data here (say 50%) instead of the entire dataset

# number of topics in Word2Vec
total_topics_word2vec <- 10

# Train
model <- word2vec(x = x, type = "cbow", dim = total_topics_word2vec, iter = 20)
embedding <- as.matrix(model)
``` 

Step 2 -  Construct variables from word embeddings

Similar to tutorial 1.1, we loop over all reviews. For a refresher on for loops in R you can visit: https://www.dataquest.io/blog/for-loop-in-r/. If you'd like more practice with for loops, try exercises 1, 3, 6, 9, 14, and 18 from https://www.w3resource.com/r-programming-exercises/control-structure/index.php. 

```{r, Construct variables}

# TODO: Use the other split of the data here (say 50%) instead of the entire dataset

# Create an empty matrix to store the posteriors
posteriors_w2v <- matrix(0, nc = total_topics_word2vec, nr = total_reviews)

# Loop over all reviews
for (k in 1:total_reviews )
{
   # 2.1 get a review and tokenize it - identify the words, separately
   tokenized_review <- unlist(strsplit(reviews_raw$full_text[[k]],"[^a-zA-Z0-9]+"))

   # 2.2 get the word vectors per review using predict()
   embedding_review <- predict(model, tokenized_review, type = "embedding")

  # 2.3 compute mean across all words in the review 
   posteriors_w2v[k,] <- apply(embedding_review, 2, mean, na.rm=TRUE)
}
``` 

Tip: for the above data splits, mind the time. Best to train in a split that temporarily precedes the prediction split 

Step 3 - Use the constructed variables to forecast

```{r, Forecast}

# prepare the constructed variables for analysis
log_BO   <- log(as.numeric(gsub(",", "",reviews_raw$first_week_box_office)))
data_reg <- cbind(c(log_BO), posteriors_w2v)
colnames(data_reg) <- c("LogBoxOffice", paste("w2v_", as.character(1:total_topics_word2vec), sep=""))

# forecast
w2v_BO_lm<- lm(LogBoxOffice ~ posteriors_w2v,  data=as.data.frame(data_reg))
summary(w2v_BO_lm)


# write.csv(data_reg, "data_reg.csv")

```


