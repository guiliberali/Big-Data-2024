{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c3933d-cc8a-4288-b1c0-5da718ae542b",
   "metadata": {},
   "source": [
    "# Learning from Big Data: Module 1 - Final Assignment Template\n",
    "\n",
    "#### Student Name: FIRSTNAME SECONDNAME (000000XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592fa644-fe0c-4e3b-aa36-d220d7337764",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e580859-86f2-4156-995b-59938c2e3a62",
   "metadata": {},
   "source": [
    "This file provides a template for **Assignment 1** from the Learning from Big Data course. This Jupyter Notebook file was prepared to save you time so that you can focus on the theory and technical parts of the methods seen in class. This was prepared with a specific application in mind: *movie reviews*. For the supervised learning tasks, we will focus on three topics: acting, storyline, and visual/sound effects.\n",
    "\n",
    "You have by now received the dataset of reviews, the three dictionaries with the training set of words for each topic, a list of stopwords, and a validation dataset containing sentences classified by a panel of human judges. This Jupyter Notebook file has a lot of (Python) code written to handle things such as leading these data files and general settings of the environment we use to perform the analysis. The supervised learning code in this file was covered in **Session 02**.\n",
    "\n",
    "This Jupyter Notebook file will load all the above-mentioned files and make them available for you to use them for solving the NLP problems listed here. The questions **you are to answer** are marked as \"`QUESTION`\". The parts **you are expected to code yourself** are marked as \"`# ADD YOUR CODE HERE`\". There, you are expected to write your own code based on the in-class discussions and the decisions you will make as you study the theory, material, and models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a700a-ecff-4703-b5f6-6cf177018095",
   "metadata": {},
   "source": [
    "#### This assignment has the following structure:\n",
    "1. **General Guidelines**\n",
    "2. **Research Question**\n",
    "3. **Load the Packages**\n",
    "4. **Load the Reviews**\n",
    "5. **Data Aggregation and Formatting**\n",
    "6. **Supervised Learning: The Naive Bayes Classifier (NBC)**\n",
    "7. **Supervised Learning: Inspect the NBC Performance**\n",
    "8. **Unsupervised Learning: Predicting the Box Office using LDA**\n",
    "9. **Unsupervised Learning: Predicting the Box Office using Word2Vec**\n",
    "10. **Analysis - Answering the Research Question**\n",
    "11. **OPTIONAL - Run and interpret the VADER lexicon for sentiment**\n",
    "12. **APPENDIX**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34c54a-0507-4507-84d8-11077b7ecb6f",
   "metadata": {},
   "source": [
    "# 1. General Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675b7a6-9a9b-40d1-b854-385a651e5a35",
   "metadata": {},
   "source": [
    "**Page limit**. This template has 8 pages, and you are allowed to add 8 to 10 pages (not including the appendix). Even though there is a page limit, you have the possibility of using appendices, which do not have a limitation in the number of pages. Use your pages wisely. For example, having a table with 2 rows and 3 columns that uses 50% (or even 25%) of a page is not really wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905274a-44dc-4cb3-b777-869d7d4dddb5",
   "metadata": {},
   "source": [
    "# 2. Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d757a-86ff-4b28-a967-106867a585b2",
   "metadata": {},
   "source": [
    "`QUESTION I:` Present here the main research question you are going to answer with your text analysis.\n",
    "You are free to choose the problem and change it until the last minute before handing in your report. However, your question should not be so simple that it does not require text analysis. For example, if your question can be answered by reading two reviews, you do not need text analysis; all you need is 10 seconds to read two reviews. Your question should not be so difficult that you cannot answer in your report. Your question needs to be answered in these pages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bda564-933f-4635-bdf8-4f8433a0ed76",
   "metadata": {},
   "source": [
    "`SOLUTION I:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad0d6d-9027-44c0-a5c2-6fc68fdd25c4",
   "metadata": {},
   "source": [
    "# 3. Load the Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb610e-8067-48c0-bbe9-42854f08f160",
   "metadata": {},
   "source": [
    "Before starting the problem set, make sure that you have all the required packages installed properly. Simply run the code cell below (Shift-Enter). **Note**: you are free to add other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f4aaa19-c7e8-4a7b-a8d2-42c2a4d21938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required packages\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, namedtuple\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fffeb-b36d-4362-ae78-8a1fc44dedcc",
   "metadata": {},
   "source": [
    "# 4. Load the Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f00046-9097-4e32-a2d7-29229c16d84c",
   "metadata": {},
   "source": [
    "We will explore the concepts from this problem set with a dataset of online movie reviews. The reviews were written from 2009 to 2013. The data was collected in 2014. Each observation in the data includes the textual review, a numerical rating from 1 to 10 (i.e., the number of stars), the movie title, the reviewer, and the date the review was written. The observation includes data from the moving being reviewed: the movie release data, the box office in the first week (as that is the strongest predictor of movie success), the studio that produced the movie, the number of theaters that the movie was released in, and the MPAA rating. The review also includes two pieces of information on the quality of the review itself: the number of readers who found the review useful, and the number of readers who rated the review as useful or not useful. There are reviews that no one rate as useful or not useful. The date in which a review was rated is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23532f2-050f-452a-94cf-06d511c96298",
   "metadata": {},
   "source": [
    "### The data set contains the following 19 columns:\n",
    "+ **movie_name**: title of the movie being reviewed.\n",
    "+ **review_code**: a unique serial identifier for all the reviews in this dataset.\n",
    "+ **reviewer**: the reviewer who wrote the review.\n",
    "+ **num_eval**: the number of stars.\n",
    "+ **review_date**: the date the review was written.\n",
    "+ **prob_sentiment**: a placeholder variable to store the probability the review is positive. `TODO`: You need to compute this.\n",
    "+ **words_in_lexicon_sentiment_and_review**: the number of words that are found both in the review and in the sentiment lexicon you will be using.\n",
    "+ **ratio_helpful**: number of people that rated the review as useful divided by the total number of people that rated the review.\n",
    "+ **raters**: number of people that rated the review as either useful or not useful.\n",
    "+ **prob_storyline**: a placeholder variable to store the probability the review is about the movie storyline.\n",
    "+ **prob_acting**: a placeholder variable to store the probability the review is about acting.\n",
    "+ **prob_sound_visual**: a placeholder variable to store the probability that the review is about the movie special effects (sound or visual).\n",
    "+ **full_text**: raw review text.\n",
    "+ **processed_text**: the cleaned review text, free of punctuation marks.\n",
    "+ **release_date**: the day the movie was released.\n",
    "+ **first_week_box_office**: number of movie theaters tickets sold in the first week from movie release. Data from boxofficemojo.com\n",
    "+ **MPAA**: MPAA rating of the movie (e.g., PG-rated).\n",
    "+ **studio**: movie studio that produced the movie.\n",
    "+ **num_theaters**: number of movie theaters that this movie was shown on the release date. Data from boxofficemojo.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d782cb1-995a-438b-9d18-ac2560da8515",
   "metadata": {},
   "source": [
    "### Loading the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9e96352-fe9e-4c75-9710-3f7b2315dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "reviews_raw = pd.read_csv('../data/reviews/reviews_tiny.csv', encoding='ISO-8859-1')\n",
    "reviews_raw = reviews_raw[\n",
    "    ['movie_name',\n",
    "     'review_code',\n",
    "     'reviewer',\n",
    "     'review_date',\n",
    "     'num_eval',\n",
    "     'prob_sentiment',\n",
    "     'words_in_lexicon_sentiment_and_review',\n",
    "     'ratio_helpful',\n",
    "     'raters',\n",
    "     'prob_storyline',\n",
    "     'prob_acting',\n",
    "     'prob_sound_visual',\n",
    "     'full_text',\n",
    "     'processed_text',\n",
    "     'release_date',\n",
    "     'first_week_box_office',\n",
    "     'MPAA',\n",
    "     'studio',\n",
    "     'num_theaters']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4827ae7a-b3c3-49e1-b1a0-db75d5404475",
   "metadata": {},
   "source": [
    "# 5. Data Aggregation and Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6f6c9-60cc-4e51-88ba-5ba160bd235f",
   "metadata": {},
   "source": [
    "`QUESTION II:` Decide on how to aggregate or structure your data. The data you received is at the review level (i.e., each row/observation is a review). However, the variablese in the data are very rich and allow you to use your creativity when designing your *research question*. For example, there are timestamps, which allow you to aggregate the data at the daily level (or even hourly level). There is information on reviewers, which allow you to inspect patterns of rating by reviewers. There is information on the studio's, and more. Please explicitly indicate how you structured your dataset, and what is your motivation to do so. Even if you are using the data at the review level, indicate how and why that is needed for your research question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af5c2e-27cf-4af0-b1ec-306b5dbe77ba",
   "metadata": {},
   "source": [
    "`SOLUTION II:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb510ef1-a216-4950-9b1a-8431a2eb6750",
   "metadata": {},
   "source": [
    "# 6. Supervised Learning: the Naive Bayes Classifier (NBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07564b4-0ffa-4442-badb-786737034bd7",
   "metadata": {},
   "source": [
    "## 6.0 Load Support Functions and Global Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e62f4-0412-42c0-83ca-39ff3a240434",
   "metadata": {},
   "source": [
    "The two functions, `compute_posterior_sentiment` and `compute_posterior_content`, are called once per review. These functions use the Bayes rule we saw in **Session 02** to compute the posterior probabilities that the review is about each topic (in the 2nd function) and the posterior probability that the sentiment in the review is positive and/or negative (in the 1st function). The functions are loading by executing the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca5f54d-3c14-4ee4-b23b-caa26da78be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for computing posterior sentiment\n",
    "def compute_posterior_sentiment(prior, corpus_in, dict_words, p_w_given_c, TOT_DIMENSIONS):\n",
    "    prior = np.array(prior)\n",
    "    vec = CountVectorizer(vocabulary=dict_words, lowercase=True)\n",
    "    word_matrix = vec.fit_transform([corpus_in]).toarray()\n",
    "\n",
    "    # Check if there are any relevant words in the review, if there are, treat them. \n",
    "    # If not, use the prior.\n",
    "    if word_matrix.sum() == 0:\n",
    "        posterior = prior\n",
    "        words_ = ['']\n",
    "    else:\n",
    "        # Positions in word matrix that have words from this review\n",
    "        word_matrix_indices = np.where(word_matrix > 0)[1]\n",
    "\n",
    "        # Initializing posterior vector\n",
    "        posterior = np.zeros(TOT_DIMENSIONS)\n",
    "        vec_likelihood = np.zeros(TOT_DIMENSIONS)\n",
    "\n",
    "        # Loop around words found in review\n",
    "        for word_matrix_index in word_matrix_indices:\n",
    "            word = vec.get_feature_names_out()[word_matrix_index]\n",
    "\n",
    "            # Check if the word exists in p_w_given_c.words\n",
    "            p_w_given_c_indices = np.where(p_w_given_c.words == word)[0]\n",
    "            if p_w_given_c_indices.size > 0:\n",
    "                p_w_given_c_index = p_w_given_c_indices[0]\n",
    "                vec_likelihood = np.array([p_w_given_c.pos_likelihood[p_w_given_c_index], \n",
    "                                           p_w_given_c.neg_likelihood[p_w_given_c_index]])\n",
    "\n",
    "                # Looping around occurrences | word\n",
    "                for i in range(word_matrix[0, word_matrix_index]):\n",
    "                    numerat = prior * vec_likelihood\n",
    "                    denomin = prior.dot(vec_likelihood)\n",
    "                    posterior = numerat / denomin\n",
    "\n",
    "                    if np.sum(posterior) > 1.01:\n",
    "                        raise Exception('ERROR')\n",
    "\n",
    "                    prior = np.array(posterior)\n",
    "\n",
    "        words_ = vec.get_feature_names_out()[word_matrix_indices]\n",
    "\n",
    "    return {'posterior_': posterior, 'words_': words_}\n",
    "\n",
    "\n",
    "# Function for computing posterior content\n",
    "def compute_posterior_content(prior, corpus_in, dict_words, p_w_given_c, BIGRAM, TOT_DIMENSIONS):\n",
    "    vec = CountVectorizer(vocabulary=dict_words, lowercase=True, ngram_range=(1, BIGRAM))\n",
    "    word_matrix = vec.fit_transform([corpus_in]).toarray()\n",
    "\n",
    "    # Check if there are any relevant words in the review, if there are, treat them. \n",
    "    # If not, use the prior.\n",
    "    if word_matrix.sum() == 0:\n",
    "        posterior = prior\n",
    "    else:\n",
    "        # Positions in word matrix that have words from this review\n",
    "        word_matrix_indices = np.where(word_matrix > 0)[1]\n",
    "        posterior = np.zeros(TOT_DIMENSIONS)\n",
    "\n",
    "        # Loop around words found in review\n",
    "        for word_matrix_index in word_matrix_indices:\n",
    "            word = vec.get_feature_names_out()[word_matrix_index]\n",
    "            p_w_given_c_index = np.where(p_w_given_c.words == word)[0][0]\n",
    "            vec_likelihood = np.array([p_w_given_c.storyline[p_w_given_c_index], \n",
    "                                       p_w_given_c.acting[p_w_given_c_index], \n",
    "                                       p_w_given_c.visual[p_w_given_c_index]])\n",
    "\n",
    "             # Looping around occurrences | word\n",
    "            for i in range(word_matrix[0, word_matrix_index]):\n",
    "                numerat = prior * vec_likelihood\n",
    "                denomin = prior.dot(vec_likelihood)\n",
    "                posterior = numerat / denomin\n",
    "\n",
    "                if np.sum(posterior) > 1.01:\n",
    "                    raise Exception('ERROR')\n",
    "\n",
    "                prior = posterior\n",
    "\n",
    "    return {'posterior_': posterior}\n",
    "\n",
    "\n",
    "# Setting Global Parameters\n",
    "PRIOR_SENT = 1/2\n",
    "PRIOR_CONTENT = 1/3\n",
    "TOT_REVIEWS = len(reviews_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b4aec-34b2-4d29-ae89-d6f535d18f70",
   "metadata": {},
   "source": [
    "## 6.1 Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad200a5-9390-4135-b36d-2d79ebc2a4dd",
   "metadata": {},
   "source": [
    "`QUESTION III:` Create the content likelihoods based on the 3 lists of words below. Be explicit on the decisions you took in the process, and why you made those decisions (e.g., which smoothing approach you used)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d062228-667c-47ca-bb42-f2b797ebc308",
   "metadata": {},
   "source": [
    "`SOLUTION III:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3476e4-c916-409c-894c-2876b1f24709",
   "metadata": {},
   "source": [
    "### 6.1.1 Loading the Dictionaries (Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0b3c994-e57c-4e32-b0b4-2fed8c1101b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the storyline dictionary\n",
    "dictionary_storyline = pd.read_csv('../data/lexicons/storyline_33k.txt')\n",
    "\n",
    "# Loading the acting dictionary\n",
    "dictionary_acting = pd.read_csv('../data/lexicons/acting_33k.txt')\n",
    "\n",
    "# Loading the visual dictionary\n",
    "dictionary_visual = pd.read_csv('../data/lexicons/visual_33k.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bcaba-56ef-4bab-abe5-ef1b8b213a9f",
   "metadata": {},
   "source": [
    "### 6.1.2 Content/Topic Likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c241fbf-a583-4488-8b53-5e24b621cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE, replacing these fake likelihoods\n",
    "likelihoods_content = pd.read_csv('../data/lexicons/example_100_fake_likelihood_content.csv')\n",
    "\n",
    "# Converting the first column to a list of strings\n",
    "lexicon_content = likelihoods_content.iloc[:, 0].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce0617-dc67-4aff-ae9f-e6fc29fb7991",
   "metadata": {},
   "source": [
    "### 6.1.3 Sentiment Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128895d-32ce-41b9-8eab-db8a7bd0d0a9",
   "metadata": {},
   "source": [
    "`QUESTION IV:` Locate a list of sentiment words that fits your research question. For example, you may want to look just at positive and negative sentiment (hence two dimensions), or you may want to look at other sentiment dimensions, such as specific emotions (excitement, fear, etc.).\n",
    "**TIP:** Google will go a long way for finding these, but do check if there is a paper you can cite that uses your list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da311c-46f4-452f-a23d-39f2367fe457",
   "metadata": {},
   "source": [
    "`SOLUTION IV:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2844c305-12ee-4d1a-b009-4a0940420cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE, load your sentiments words list\n",
    "# dictionary_sentiment = pd.read_csv('')\n",
    "\n",
    "# ADD YOUR CODE HERE, replacing these fake likelihoods\n",
    "likelihoods_sentiment = pd.read_csv('../data/lexicons/example_100_fake_likelihood_sentiment.csv')\n",
    "\n",
    "# Converting the first column to a list of strings\n",
    "lexicon_sentiment = likelihoods_sentiment.iloc[:, 0].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04057e5-81fb-4d64-bde9-61bd0185340b",
   "metadata": {},
   "source": [
    "## 6.2 Run NBC for Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d45f1a3-9897-4867-958a-e356893b883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentiment of review #0\n",
      "Computing sentiment of review #100\n",
      "Computing sentiment of review #200\n",
      "Computing sentiment of review #300\n",
      "Computing sentiment of review #400\n",
      "Computing sentiment of review #500\n",
      "Computing sentiment of review #600\n",
      "Computing sentiment of review #700\n",
      "Computing sentiment of review #800\n",
      "Computing sentiment of review #900\n"
     ]
    }
   ],
   "source": [
    "for review_index in range(TOT_REVIEWS):\n",
    "    if (review_index % 100 == 0):\n",
    "        print(f\"Computing sentiment of review #{review_index}\")\n",
    "        \n",
    "     \n",
    "    # Reset the prior as each review is looked at separately\n",
    "    prior_sent = [PRIOR_SENT, 1-PRIOR_SENT]\n",
    "\n",
    "    text_review = str(reviews_raw['processed_text'].iloc[review_index])\n",
    "\n",
    "    # Pre-process the review to remove punctuation marks and numbers\n",
    "    # Note: we are not removing stopwords here (nor elsewhere - a point for improvement)\n",
    "    text_review = text_review.translate(str.maketrans('', '', string.punctuation))\n",
    "    text_review = ''.join([i for i in text_review if not i.isdigit()])\n",
    "\n",
    "    # Computing posterior probability the review is positive\n",
    "    TOT_DIMENSIONS = 2\n",
    "    sent_results = compute_posterior_sentiment(prior=prior_sent,\n",
    "                                               corpus_in=text_review,\n",
    "                                               dict_words=lexicon_sentiment,\n",
    "                                               p_w_given_c=likelihoods_sentiment,\n",
    "                                               TOT_DIMENSIONS=TOT_DIMENSIONS)\n",
    "    \n",
    "    words_sent = sent_results['words_']\n",
    "    posterior_sent = sent_results['posterior_']\n",
    "\n",
    "    # Setting the posterior sentiment in the prob_sentiment column\n",
    "    reviews_raw.loc[review_index, 'prob_sentiment'] = posterior_sent[0]\n",
    "    reviews_raw.loc[review_index, 'words_in_lexicon_sentiment_and_review'] = ' '.join(words_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb237f9-c651-4723-8e94-4dd7aab75207",
   "metadata": {},
   "source": [
    "## 6.3 Run NBC for Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e19abd2-ef7f-4bbf-beca-9e5d27498585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing content of review # 0\n",
      "Computing content of review # 100\n",
      "Computing content of review # 200\n",
      "Computing content of review # 300\n",
      "Computing content of review # 400\n",
      "Computing content of review # 500\n",
      "Computing content of review # 600\n",
      "Computing content of review # 700\n",
      "Computing content of review # 800\n",
      "Computing content of review # 900\n"
     ]
    }
   ],
   "source": [
    "for review_index in range(TOT_REVIEWS):\n",
    "    print(f'Computing content of review # {review_index}') if review_index%100 == 0 else None\n",
    "    \n",
    "    if reviews_raw['full_text'].iloc[review_index] != \"\":\n",
    "        text_review = str(reviews_raw['processed_text'].iloc[review_index])\n",
    "\n",
    "        # Pre-process the review to remove punctuation marks and numbers\n",
    "        # Note: we are not removing stopwords here (nor elsewhere - a point for improvement)\n",
    "        text_review = text_review.translate(str.maketrans('', '', string.punctuation))\n",
    "        text_review = ''.join([i for i in text_review if not i.isdigit()])\n",
    "        \n",
    "        # Compute posterior probability the review is about each topic/content\n",
    "        TOT_DIMENSIONS = 3\n",
    "        prior_content = np.repeat(PRIOR_CONTENT, TOT_DIMENSIONS).reshape(-1, TOT_DIMENSIONS)\n",
    "        posterior_content = compute_posterior_content(prior=prior_content, \n",
    "                                              corpus_in=text_review,\n",
    "                                              dict_words=lexicon_content,\n",
    "                                              p_w_given_c=likelihoods_content, \n",
    "                                              BIGRAM=2,\n",
    "                                              TOT_DIMENSIONS=TOT_DIMENSIONS)\n",
    "        \n",
    "        reviews_raw.loc[review_index, 'prob_storyline'] = posterior_content['posterior_'][0][0]\n",
    "        reviews_raw.loc[review_index, 'prob_acting'] = posterior_content['posterior_'][0][1]\n",
    "        reviews_raw.loc[review_index, 'prob_sound_visual'] = posterior_content['posterior_'][0][2]\n",
    "\n",
    "processed_reviews = reviews_raw\n",
    "\n",
    "# Save the updated file, now including the sentiment and content/topic posteriors.\n",
    "processed_reviews.to_csv('../output/test_processed_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e46a97-62f9-41e7-916b-754b0ae4238c",
   "metadata": {},
   "source": [
    "# 7. Supervised Learning: Inspect the NBC Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f721df9-58de-4e74-b7c5-ad0449c2cc42",
   "metadata": {},
   "source": [
    "## 7.1 Load the Judge Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b12176de-180e-4295-ad05-d929b51b7c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the judges scores\n",
    "ground_truth_judges = pd.read_csv('../data/judges/judges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa88067-1ea3-4c06-8fe3-f4da97ecbe60",
   "metadata": {},
   "source": [
    "## 7.2 Compute Confusion Matrix, Precision, and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cec1a9-5906-4758-8117-a5dd6efe79ab",
   "metadata": {},
   "source": [
    "`QUESTION V:` Compare the performance of your NBC implementation (for content) against the judges ground truth by building the confusion matrix and computing the precision and accuracy scores. **Do not forget to interpret your findings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "734d44bb-e16f-42f2-9ecb-8135eaa236b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff461502-a62d-4a20-a1d5-9f8467c77db1",
   "metadata": {},
   "source": [
    "`SOLUTION V:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094acf09-939d-45ff-a9a5-23506df6e5ab",
   "metadata": {},
   "source": [
    "# 8. Unsupervised Learning: Predicting Box Office using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10f03b-e88b-4fed-b3dd-5326ba9a3a33",
   "metadata": {},
   "source": [
    "`QUESTION VI:` Using Latent Dirichlet Allocation (LDA), predict the movie box office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08388bf7-f3d3-4606-aa26-d69dcfb0efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE. Note: you are allowed to use the code from Session 03."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade221f-a203-4021-925f-788bb3a7896b",
   "metadata": {},
   "source": [
    "`SOLUTION VI:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae7495-1dee-4ba3-8a08-b1faa5d13320",
   "metadata": {},
   "source": [
    "# 9. Unsupervised Learning: Predicting the Box Office using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2512cda3-f0b2-4693-8987-aab0d6ab2b44",
   "metadata": {},
   "source": [
    "`QUESTION VII:` Using Word2Vec, predict movie box office.\n",
    "+ **Tip 1:** You can reduce the dimensionality of the output of Word2Vec with PDA/Factor Analysis. This will save you computing time.\n",
    "+ **Tip 2:** Word2Vec wil give you word vectors. You can then compute the average of these word vectors for all words in a review. This will give you vector describing the content of a review, which you can use as your constructed variable(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b1dff43-7d5f-47af-ba27-972ea32ab504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE. Note: you are allowed to use the code from Session 03."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a83e3-1f6c-42a8-a0fe-84ef40db06e9",
   "metadata": {},
   "source": [
    "`SOLUTION VII:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943293af-4093-4984-921b-4d9c8226bd74",
   "metadata": {},
   "source": [
    "# 10. Analysis: Use the constructed variables to answer your research question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af398c-c5b0-4ee6-b64a-548071cd52ca",
   "metadata": {},
   "source": [
    "`QUESTION VIII:` Now that you have constructed your NLP variables for sentiment and content using both supervised and unsupervised methods, use them to answer your original research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "040dda02-64e8-4be7-8b1f-be5e6e80ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE. The code for the analysis of your research question should be written here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ea384-f220-4ecc-a0dc-efc88dbd4d0a",
   "metadata": {},
   "source": [
    "`SOLUTION VIII:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c7a5c-63ba-4a0f-b313-8bc50d0272c1",
   "metadata": {},
   "source": [
    "# OPTIONAL: Run and interpret sentiment with the supervised learning VADER lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9cc3b-1f44-49c0-a55b-2a74c97af14a",
   "metadata": {},
   "source": [
    "`QUESTION IX (optional):` Using the VADER code you received in the lecture, compute the sentiment using the VADER package. Compare the performance of your NBC implementation (for sentiment) assuming that the VADER classification were the ground truth and then build the confusion matrix, compute the precision, and computre the recall. **Note** that we are now interested in understanding how much the two classifications differ and how, but we are not implying that VADER is error-free, far from it. We are interested in uncovering sources of systemic differences that can be attributed to the algorithms or lexicons. **Do interpret your findings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "154b3a6b-8c8c-432e-858a-3fb3da7c6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca24d3-0ef6-44d3-8321-edba0eff360b",
   "metadata": {},
   "source": [
    "`SOLUTION IX (optional):`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644eb11f-f85d-4c83-99a9-9224b8ca53a8",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b9a2d-acdd-498d-a5e5-7d2ede8e3ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
